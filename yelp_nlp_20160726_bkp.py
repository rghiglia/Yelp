# -*- coding: utf-8 -*-
"""
Created on Mon Jul 25 13:08:44 2016

@author: rghiglia
"""

# Packages
import sys
import time
import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sb


# Custom packages
sys.path.append(r'C:\Users\rghiglia\Documents\ML_ND\Toolbox')
from rg_toolbox_data import df_summarize

# ----------------------------------------------------------------------------
# Extract Data
# ----------------------------------------------------------------------------
# See Yelp.ptx for data structure

dnm = r'C:\Users\rghiglia\Documents\ML_ND\Yelp'

# Assign data to dataframes
# It will create 'dfs', a list of data frames containing 5 datasets
# call them with dfs['business']

import glob
fnm = glob.glob(dnm + '\\' + '*.csv')
nm_df = [f.replace(dnm,'').replace('\\yelp_academic_dataset_','').replace('.csv','') for f in fnm]
dfs = {nm: None for nm in nm_df}
for nm, f in zip(nm_df, fnm):
    t0 = time.time()
    print "Processing file '{0}' ...".format(nm)
    dfs[nm] = pd.read_csv(f, dtype=object)
    print("\t\t\t\t %1.2f s" % (time.time() - t0))
nR = {nm: 0 for nm in nm_df}    # of rows
nC = {nm: 0 for nm in nm_df}    # of columns



# ----------------------------------------------------------------------------
# Looking at reviews
# ----------------------------------------------------------------------------

df_rev = dfs['review']
df_rev.info()
#df_rev['votes.funny'].unique()
# I guess that is how other customers view the reviews

nm0 = 'business'
df = dfs[nm0].copy()    # may be inefficient but I might change the data
#nR[nm0], nC[nm0] = df.shape
#print "Data '{0}' has {1} rows x {2} columns".format(nm0, nR[nm0], nC[nm0])
#df_bus_smry = df_summarize(df)
#print df_bus_smry
# 77k entries, with 98 columns
# This is a good way to summarize data:
# Fisrt column is the column name
# Second column is how many non-null entries
# Third column is how many distinct values it has

# Notes:
# busID is unique nB IDs
# things like full_address:
# all have an entry but not nB uniques
# since they are all different businesses, either you have two businesses 
# at the same address or there might be a default value that is not a null

# There are 9710 categories


# I want to know how many entries we have for each category

# I think that's a groupby thingy
#grp = pd.groupby(df, 'categories')
#df_tmp = grp.count()
#df_tmp.info()

# Much better implementation
#grp_cat = df.groupby('categories').size().sort_values(ascending=False)
#print grp_cat
#
## I see, so the value for category is actually a multiple thing already:
#df.ix[0,'categories']
#
# I'd like to know how many entries we have per category
grp = pd.value_counts(df['categories'])     # type(grp) # Series
df_grp = pd.DataFrame({'cat': grp.index, 'cnt': grp.values})
#print df_grp

## Isn't this simpler? Well, it's the same thing just with more labels
#df_num['stars'].value_counts()


# Separate out the categories
catU = []
for cats in df_grp['cat']:
    catss = [wrd.strip() for wrd in re.split(',',re.sub('[\[\]\']', '', cats))] # removes "'" "[" and "]", then splits by comma then trims each entry in the list
    catU += catss
catU = (list(set(catU)))
catU.sort()

# Extract only restaurants
ixRestaurants = []
for (i, cats) in enumerate(df['categories']):
    catss = [wrd.strip() for wrd in re.split(',',re.sub('[\[\]\']', '', cats))]
    if 'Restaurants' in catss:
        ixRestaurants.append(i)
len(ixRestaurants)
# 25,071 restaurants

df_rev = dfs['review'].iloc[ixRestaurants].copy()
df_rev.info()
print df_summarize(df_rev)
# hmmm, vote.useful has 26 distinct values ...
#df_rev['votes.useful']
# Ahh, I guess it's how many people found it useful
# I think you want to convert it into groups, say 5, or maybe 2
#print df_rev['votes.useful']
#type(df_rev['votes.useful'])
#print df_rev['votes.useful'].hist()
## Error: cannot concatenate 'str' and 'float' object
#any(df_rev['votes.useful'].isnull())
## Damn
df_rev['votes.useful'] = pd.Series([int(x) for x in df_rev['votes.useful']], index=df_rev.index)
df_rev['votes.useful'].hist(bins=30)
len(df_rev['votes.useful'].ix[df_rev['votes.useful']>0])


# To do:
# 0. Restrict to restaurant businesses
# 1. Figure out how to deal with multiple reviews
# 2. Boil down a significative vocabulary, i.e. dense and parsimonious
# 3. Try to define a question

# Q: Are useful reviews identifiable?
# In ML framework, can you predict usefulness given review?

# It could be that it is not the choice of words themselves but the concept
# generated by the words. That seems to be likely, but maybe there is something
# there. Maybe the diversity and variety of language is explanatory

# To answer the question you need to:
# Turn reviews into numerical representations (sparse matrices)
# Pick output variable: votes.useful


# Split sample

# Decide output variable
out_varble = 'votes.useful'
y = df_rev[out_varble].copy()
y[y>=1] = 1
nO = len(df_rev)
nTst_tmp = int(nO*0.15)

from sklearn.cross_validation import StratifiedShuffleSplit
sss0 = StratifiedShuffleSplit(y, 1, test_size=nTst_tmp-1, random_state=0)
for ix_trn, ix_tst in sss0:
    pass
nTrn = len(ix_trn)
nTst = len(ix_tst)
if nO!=nTrn+nTst: print "Error: training and test don't sum up"

# Re-assign for correct indexing in original data frame
ix_trn = y.index[ix_trn]
ix_tst = y.index[ix_tst]
y_trn = y.ix[ix_trn]
y_tst = y.ix[ix_tst]
#X_trn, y_trn = X.ix[ix_trn], y.ix[ix_trn]
#X_tst, y_tst = X.ix[ix_tst], y.ix[ix_tst]
#any(y.isnull())
#any(y_trn.isnull())
## Ok

# Sets for CV
pctCV_tst = 0.15 # CV test
nCV_tst_tmp = int(nTrn*pctCV_tst)

nCV = 3
sss = StratifiedShuffleSplit(y_trn, nCV, test_size=nCV_tst_tmp, random_state=1)
for i, (train_index, test_index) in enumerate(sss):
    if i==0:
        nCV_trn = len(train_index)
        nCV_tst = len(test_index)
    else:
        break

if nTrn!=nCV_trn+nCV_tst: print "Error: training and test don't sum up"

# Within sss in iloc coordinates
Ix_trn = np.zeros((nCV_trn, nCV))
Ix_tst = np.zeros((nCV_tst, nCV))
for i, (train_index, test_index) in enumerate(sss):
    Ix_trn[:,i] = y_trn.index[train_index]
    Ix_tst[:,i] = y_trn.index[test_index]
    Ix_trn[:,i] = y_trn.index[train_index]
    Ix_tst[:,i] = y_trn.index[test_index]
# Still problems!
# Finally!
Ix_trn = Ix_trn.astype(int)
Ix_tst = Ix_tst.astype(int)

# Select CV train and CV test sets
i0 = 0 # e.g. select first fold

#X_CV_trn, y_CV_trn = X_trn.ix[Ix_trn[:,i0]], y_trn.ix[Ix_trn[:,i0]]
#X_CV_tst, y_CV_tst = X_trn.ix[Ix_tst[:,i0]], y_trn.ix[Ix_tst[:,i0]]

y_CV_trn = y_trn.ix[Ix_trn[:,i0]]
y_CV_tst = y_trn.ix[Ix_tst[:,i0]]

df_rev_CV_trn = df_rev.ix[Ix_trn[:,i0]]
df_rev_CV_tst = df_rev.ix[Ix_tst[:,i0]]

# Start with pre-processing texts
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

w_stp = set(stopwords.words('english')) # because it will be used in a loop and set() is faster than list()
stemmer = PorterStemmer()

def clean_review(rev_raw):
    rev = BeautifulSoup(rev_raw, 'lxml').get_text() # clean from HTML
    rev = re.sub('[^a-zA-Z]', ' ', rev.lower()) # remove punctuation, numbers, etc. and make it lower case
    wrds = rev.split()
    wrds = [stemmer.stem(w) for w in wrds if not w in w_stp] # discard most common words and stem
    return(' '.join(wrds))

# Example
rev_cln = clean_review(df_rev_CV_trn['text'].iloc[0])


nR = len(df_rev_CV_trn)
rev_cln = []
for i in range(nR):
    print 'Review {}/{}'.format(i+1, nR)
    rev_cln.append(clean_review(df_rev['text'].iloc[i]))


# Bag of Words
from sklearn.feature_extraction.text import CountVectorizer

#vectorizer = CountVectorizer(analyzer = 'word',   \
#                             tokenizer = None,    \
#                             preprocessor = None, \
#                             stop_words = None,   \
#                             max_features = None)


vectorizer = CountVectorizer(analyzer='word')
X_CV_trn = vectorizer.fit_transform(rev_cln)
X_CV_trn = X_CV_trn.toarray()
vocab = vectorizer.get_feature_names()
print vocab

## Inspecting data:
#data.sum(axis=1)
#df_rev['text'][7]
#"I highly recommend this place. The mechanics are really honest and I'm usually in and out with no issues."
### Note:
### 1. Only 8 significant words:
##rev_cln[7]
### 2. You will not be able to tell rating for example, it will not distinguish between 'issue' and 'no issue'

#df_rev['text'].iloc[6]

# Sum up the counts of each vocabulary word
wrd_tot = np.sum(X_CV_trn, axis=0)  # frequency of word
rev_vrb = np.sum(X_CV_trn, axis=1)  # length of review
wrd_cnt = {vocab[i]: wrd_tot[i] for i in range(len(vocab))}

## His version:
## For each, print the vocabulary word and the number of times it 
## appears in the training set
#for tag, count in zip(vocab, dist):
#    print count, tag


'''
# Inverse frequency (if needed)
import nltk
import string
import os

from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem.porter import PorterStemmer

path = '/opt/datacourse/data/parts'
token_dict = {}
stemmer = PorterStemmer()

def stem_tokens(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed

def tokenize(text):
    tokens = nltk.word_tokenize(text)
    stems = stem_tokens(tokens, stemmer)
    return stems

for subdir, dirs, files in os.walk(path):
    for file in files:
        file_path = subdir + os.path.sep + file
        shakes = open(file_path, 'r')
        text = shakes.read()
        lowers = text.lower()
        no_punctuation = lowers.translate(None, string.punctuation)
        token_dict[file] = no_punctuation
        
#this can take some time
tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')
tfs = tfidf.fit_transform(token_dict.values())
'''


from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)
clf.fit(X_CV_trn, y_CV_trn)
y_CV_pred = clf.pred(X_CV_tst)

from sklearn.metrics import confusion_matrix
Z = pd.DataFrame([y_CV_pred, y_CV_tst], index=['pred', 'act']).T
M = confusion_matrix(y_CV_tst, y_CV_pred)
print M







